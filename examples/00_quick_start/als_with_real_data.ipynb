{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Start a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Recommender\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'interactions.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_A</th>\n",
       "      <th>User_B</th>\n",
       "      <th>Interaction_Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_A  User_B  Interaction_Intensity\n",
       "0       0       1                    1.0\n",
       "1       0       2                    1.0\n",
       "2       0       3                    1.0\n",
       "3       0       7                    4.0\n",
       "4       0       8                    1.0\n",
       "5       0       9                    2.0\n",
       "6       0      10                    5.0\n",
       "7       0      14                    5.0\n",
       "8       0      23                    1.0\n",
       "9       0      28                    3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_A: integer (nullable = true)\n",
      " |-- User_B: integer (nullable = true)\n",
      " |-- Interaction_Intensity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select(\n",
    "    col(\"User_A\").alias(\"user_id\"),\n",
    "    col(\"User_B\").alias(\"item_id\"),\n",
    "    col(\"Interaction_Intensity\").alias(\"rating\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|item_id|rating|\n",
      "+-------+-------+------+\n",
      "|      0|      0|     0|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Count null values in each column\n",
    "data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there are no nulls\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 632211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct rows: 632211\n",
      "Number of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Total rows in the dataset\n",
    "total_rows = data.count()\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "\n",
    "# Count the number of distinct rows\n",
    "distinct_rows = data.dropDuplicates().count()\n",
    "print(f\"Distinct rows: {distinct_rows}\")\n",
    "\n",
    "# Calculate duplicates\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "data = data.orderBy(rand())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|item_id|rating|\n",
      "+-------+-------+------+\n",
      "|   1250|    605|   1.0|\n",
      "|    292|    693|   5.0|\n",
      "|    453|   1272|   3.0|\n",
      "|      7|      4|   4.0|\n",
      "|    868|    415|   5.0|\n",
      "|    865|    189|   1.0|\n",
      "|    747|   1092|   2.0|\n",
      "|   1207|    231|   3.0|\n",
      "|    336|   1274|   4.0|\n",
      "|    393|    435|   2.0|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 569082\n",
      "Test data count: 63157\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = data.randomSplit([0.9, 0.1], seed=42)\n",
    "# Verify split sizes\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 11:53:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/16 11:53:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/12/16 11:53:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Initialize ALS model\n",
    "als = ALS(\n",
    "    implicitPrefs=True,\n",
    "    maxIter=20,              # Number of iterations\n",
    "    regParam=0.1,            # Regularization parameter\n",
    "    rank=20,                 # Number of latent factors\n",
    "    userCol=\"user_id\",       # Column for user IDs\n",
    "    itemCol=\"item_id\",       # Column for item IDs\n",
    "    ratingCol=\"rating\",      # Column for ratings\n",
    "    coldStartStrategy=\"drop\" # Handle unseen users/items during predictions\n",
    ")\n",
    "\n",
    "# Fit the ALS model on the training data\n",
    "als_model = als.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+----------+\n",
      "|user_id|item_id|rating|prediction|\n",
      "+-------+-------+------+----------+\n",
      "|      0|    151|   5.0|  3.127446|\n",
      "|      0|    364|   1.0|  2.845566|\n",
      "|      1|    329|   2.0| 2.7982461|\n",
      "|      1|    508|   3.0| 2.8757863|\n",
      "|      1|   1003|   1.0| 2.8427062|\n",
      "|      2|    194|   1.0| 2.9449258|\n",
      "|      2|    412|   2.0| 2.9658844|\n",
      "|      2|    431|   3.0|  2.731868|\n",
      "|      2|    655|   2.0| 3.0503056|\n",
      "|      3|    140|   1.0|  2.862554|\n",
      "|      3|    266|   4.0| 2.7611225|\n",
      "|      3|    577|   1.0| 2.9654324|\n",
      "|      3|    867|   4.0| 2.6659179|\n",
      "|      3|   1244|   2.0| 2.7762034|\n",
      "|      4|    838|   5.0| 2.8890398|\n",
      "|      4|   1005|   5.0| 2.8038964|\n",
      "|      5|    732|   4.0| 2.8919232|\n",
      "|      6|    506|   3.0| 2.7655413|\n",
      "|      6|    766|   4.0| 2.5914567|\n",
      "|      6|   1066|   3.0| 2.5186863|\n",
      "+-------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "predictions = als_model.transform(test_data)\n",
    "\n",
    "# Show the predictions\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating prediction metrics: RMSE, MAE, explained variance, R-squared\n",
    "Ranking-based evaluation: MAP, NDCG, precision@K, recall@K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse : 1.4015036480446443 \n",
      " mae: 1.2079919460116295\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with RMSE and MAE\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "evaluator_mae = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)\n",
    "print(f\"rmse : {rmse} \\n mae: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_recommendations = als_model.recommendForAllUsers(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                                                                                                                      |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0      |[{557, 3.4198232}, {145, 3.3708172}, {902, 3.3346689}, {563, 3.2411604}, {804, 3.234417}, {701, 3.2271514}, {426, 3.2252626}, {1072, 3.224292}, {974, 3.2189589}, {934, 3.21756}]    |\n",
      "|2      |[{407, 3.3153088}, {1190, 3.3107648}, {515, 3.2818205}, {202, 3.265611}, {1067, 3.2291625}, {1273, 3.2221503}, {335, 3.209711}, {1126, 3.203563}, {417, 3.1885412}, {201, 3.1879504}]|\n",
      "|4      |[{902, 3.3007252}, {347, 3.239592}, {682, 3.2143164}, {356, 3.19286}, {84, 3.1797867}, {563, 3.1616046}, {1262, 3.1603162}, {557, 3.154688}, {1295, 3.153476}, {370, 3.1494038}]     |\n",
      "|7      |[{768, 3.1665711}, {525, 3.159691}, {1047, 3.1503763}, {407, 3.0860143}, {449, 3.0798695}, {908, 3.0781276}, {1289, 3.0693996}, {900, 3.0634751}, {291, 3.061996}, {874, 3.0513053}] |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_recommendations.filter(col(\"user_id\").isin([0, 2, 4, 7])).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Recommendations: Explode the nested recommendations to make it easier to evaluate: \n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "top_k_recommendations = top_k_recommendations.withColumn(\"recommendation\", explode(col(\"recommendations\"))) \\\n",
    "    .select(\"user_id\", col(\"recommendation.item_id\").alias(\"item_id\"), col(\"recommendation.rating\").alias(\"prediction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|prediction|\n",
      "+-------+-------+----------+\n",
      "|1      |902    |3.25111   |\n",
      "|1      |407    |3.20571   |\n",
      "|1      |1047   |3.1858816 |\n",
      "|1      |833    |3.1620946 |\n",
      "|1      |525    |3.1469517 |\n",
      "|1      |515    |3.1460884 |\n",
      "|1      |449    |3.1346653 |\n",
      "|1      |56     |3.1315854 |\n",
      "|1      |718    |3.1201718 |\n",
      "|1      |291    |3.1190429 |\n",
      "|0      |557    |3.4198232 |\n",
      "|0      |145    |3.3708172 |\n",
      "|0      |902    |3.3346689 |\n",
      "|0      |563    |3.2411604 |\n",
      "|0      |804    |3.234417  |\n",
      "|0      |701    |3.2271514 |\n",
      "|0      |426    |3.2252626 |\n",
      "|0      |1072   |3.224292  |\n",
      "|0      |974    |3.2189589 |\n",
      "|0      |934    |3.21756   |\n",
      "|2      |407    |3.3153088 |\n",
      "|2      |1190   |3.3107648 |\n",
      "|2      |515    |3.2818205 |\n",
      "|2      |202    |3.265611  |\n",
      "|2      |1067   |3.2291625 |\n",
      "|2      |1273   |3.2221503 |\n",
      "|2      |335    |3.209711  |\n",
      "|2      |1126   |3.203563  |\n",
      "|2      |417    |3.1885412 |\n",
      "|2      |201    |3.1879504 |\n",
      "+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_recommendations.filter(col(\"user_id\").isin([0, 1, 2])).show(n=30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Evaluation - Precision@K: 0.002076923076923077, Recall@K: 0.00424857324032974\n"
     ]
    }
   ],
   "source": [
    "# Join with Ground Truth\n",
    "joined = top_k_recommendations.join(test_data, on=[\"user_id\", \"item_id\"], how=\"inner\")\n",
    "\n",
    "# Precision@K and Recall@K\n",
    "precision_at_k = joined.count() / (top_k_recommendations.count() * 10)\n",
    "recall_at_k = joined.count() / test_data.count()\n",
    "\n",
    "print(f\"Ranking Evaluation - Precision@K: {precision_at_k}, Recall@K: {recall_at_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
